{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chenhaohua/.conda/envs/dl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=202, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from dataset_extractor import *\n",
    "import torch\n",
    "model_trained=AutoModelForSequenceClassification.from_pretrained('/data/chenhaohua/PRC_legal/bert-base-legal-chinese-epoch-8')\n",
    "tokenizer=AutoTokenizer.from_pretrained('/data/chenhaohua/PRC_legal/bert-base-chinese')\n",
    "model_trained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_inference(batchsize=30):\n",
    "    train_pair=extract(\"/data/chenhaohua/PRC_legal_dataset/data_valid.json\")\n",
    "    device=torch.device(\"cuda\",4)\n",
    "    sum_of_correct=0\n",
    "    model_trained.to(device) # model to device once\n",
    "    for i in range(0,len(train_pair['content']),batchsize):\n",
    "        end_index = min(i + batchsize, len(train_pair['content'])-1)\n",
    "        infer_batch_tokens=tokenizer(train_pair['content'][i:end_index],padding=True,return_tensors='pt').to(device)\n",
    "        output=model_trained(**infer_batch_tokens)\n",
    "        output_logits = output.logits.to(\"cpu\")#must return specific value\n",
    "        prediction=np.array(torch.argmax(output_logits,dim=1))\n",
    "        labels=np.array(train_pair[\"label\"][i:end_index])\n",
    "        sum_of_correct += int(sum(prediction==labels))\n",
    "        if i%batchsize==0 and i!=0:\n",
    "            print(f\"finished {i} data prediction\")\n",
    "    return sum_of_correct/len(train_pair['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14501 lines have been extracted\n",
      "finished 30 data prediction\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 42.00 MiB (GPU 4; 31.75 GiB total capacity; 29.93 GiB already allocated; 25.50 MiB free; 30.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/data/chenhaohua/PRC_legal/performance.ipynb 单元格 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.212.253.234/data/chenhaohua/PRC_legal/performance.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m result\u001b[39m=\u001b[39mbert_inference()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.212.253.234/data/chenhaohua/PRC_legal/performance.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "\u001b[1;32m/data/chenhaohua/PRC_legal/performance.ipynb 单元格 3\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.212.253.234/data/chenhaohua/PRC_legal/performance.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m end_index \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(i \u001b[39m+\u001b[39m batchsize, \u001b[39mlen\u001b[39m(train_pair[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.212.253.234/data/chenhaohua/PRC_legal/performance.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m infer_batch_tokens\u001b[39m=\u001b[39mtokenizer(train_pair[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m][i:end_index],padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.212.253.234/data/chenhaohua/PRC_legal/performance.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m output\u001b[39m=\u001b[39mmodel_trained(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minfer_batch_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.212.253.234/data/chenhaohua/PRC_legal/performance.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m output_logits \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m#must return specific value\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.212.253.234/data/chenhaohua/PRC_legal/performance.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m prediction\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marray(torch\u001b[39m.\u001b[39margmax(output_logits,dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1564\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1565\u001b[0m     input_ids,\n\u001b[1;32m   1566\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1567\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1568\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1569\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1570\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1571\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1572\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1573\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1574\u001b[0m )\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1578\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    613\u001b[0m         hidden_states,\n\u001b[1;32m    614\u001b[0m         attention_mask,\n\u001b[1;32m    615\u001b[0m         layer_head_mask,\n\u001b[1;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    618\u001b[0m         past_key_value,\n\u001b[1;32m    619\u001b[0m         output_attentions,\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    498\u001b[0m         hidden_states,\n\u001b[1;32m    499\u001b[0m         attention_mask,\n\u001b[1;32m    500\u001b[0m         head_mask,\n\u001b[1;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[1;32m    430\u001b[0m         head_mask,\n\u001b[1;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    433\u001b[0m         past_key_value,\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/dl/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:325\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    322\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    324\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    328\u001b[0m     query_length, key_length \u001b[39m=\u001b[39m query_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], key_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 4; 31.75 GiB total capacity; 29.93 GiB already allocated; 25.50 MiB free; 30.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "result=bert_inference()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "def bert_inference_f1score(batchsize=30):\n",
    "    train_pair=extract(\"/data/chenhaohua/PRC_legal_dataset/data_valid.json\")\n",
    "    device=torch.device(\"cuda\",4)\n",
    "    model_trained.to(device) # model to device once\n",
    "    all_predictions=[]\n",
    "    all_labels=[]\n",
    "    for i in range(0,len(train_pair['content']),batchsize):\n",
    "        end_index = min(i + batchsize, len(train_pair['content'])-1)\n",
    "        infer_batch_tokens=tokenizer(train_pair['content'][i:end_index],padding=True,return_tensors='pt').to(device)\n",
    "        output=model_trained(**infer_batch_tokens)\n",
    "        output_logits = output.logits.to(\"cpu\")#must return specific value\n",
    "        prediction=np.array(torch.argmax(output_logits,dim=1))\n",
    "        labels=np.array(train_pair[\"label\"][i:end_index])\n",
    "        all_predictions.extend(prediction)\n",
    "        all_labels.extend(labels)\n",
    "        if i%batchsize==0 and i!=0:\n",
    "            print(f\"finished {i} data prediction\")\n",
    "    macro_f1=f1_score(all_labels,all_predictions,average=\"macro\")\n",
    "    micro_f1=f1_score(all_labels,all_predictions,average=\"micro\")\n",
    "    return macro_f1,micro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14501 lines have been extracted\n",
      "finished 30 data prediction\n",
      "finished 60 data prediction\n",
      "finished 90 data prediction\n",
      "finished 120 data prediction\n",
      "finished 150 data prediction\n",
      "finished 180 data prediction\n",
      "finished 210 data prediction\n",
      "finished 240 data prediction\n",
      "finished 270 data prediction\n",
      "finished 300 data prediction\n",
      "finished 330 data prediction\n",
      "finished 360 data prediction\n",
      "finished 390 data prediction\n",
      "finished 420 data prediction\n",
      "finished 450 data prediction\n",
      "finished 480 data prediction\n",
      "finished 510 data prediction\n",
      "finished 540 data prediction\n",
      "finished 570 data prediction\n",
      "finished 600 data prediction\n",
      "finished 630 data prediction\n",
      "finished 660 data prediction\n",
      "finished 690 data prediction\n",
      "finished 720 data prediction\n",
      "finished 750 data prediction\n",
      "finished 780 data prediction\n",
      "finished 810 data prediction\n",
      "finished 840 data prediction\n",
      "finished 870 data prediction\n",
      "finished 900 data prediction\n",
      "finished 930 data prediction\n",
      "finished 960 data prediction\n",
      "finished 990 data prediction\n",
      "finished 1020 data prediction\n",
      "finished 1050 data prediction\n",
      "finished 1080 data prediction\n",
      "finished 1110 data prediction\n",
      "finished 1140 data prediction\n",
      "finished 1170 data prediction\n",
      "finished 1200 data prediction\n",
      "finished 1230 data prediction\n",
      "finished 1260 data prediction\n",
      "finished 1290 data prediction\n",
      "finished 1320 data prediction\n",
      "finished 1350 data prediction\n",
      "finished 1380 data prediction\n",
      "finished 1410 data prediction\n",
      "finished 1440 data prediction\n",
      "finished 1470 data prediction\n",
      "finished 1500 data prediction\n",
      "finished 1530 data prediction\n",
      "finished 1560 data prediction\n",
      "finished 1590 data prediction\n",
      "finished 1620 data prediction\n",
      "finished 1650 data prediction\n",
      "finished 1680 data prediction\n",
      "finished 1710 data prediction\n",
      "finished 1740 data prediction\n",
      "finished 1770 data prediction\n",
      "finished 1800 data prediction\n",
      "finished 1830 data prediction\n",
      "finished 1860 data prediction\n",
      "finished 1890 data prediction\n",
      "finished 1920 data prediction\n",
      "finished 1950 data prediction\n",
      "finished 1980 data prediction\n",
      "finished 2010 data prediction\n",
      "finished 2040 data prediction\n",
      "finished 2070 data prediction\n",
      "finished 2100 data prediction\n",
      "finished 2130 data prediction\n",
      "finished 2160 data prediction\n",
      "finished 2190 data prediction\n",
      "finished 2220 data prediction\n",
      "finished 2250 data prediction\n",
      "finished 2280 data prediction\n",
      "finished 2310 data prediction\n",
      "finished 2340 data prediction\n",
      "finished 2370 data prediction\n",
      "finished 2400 data prediction\n",
      "finished 2430 data prediction\n",
      "finished 2460 data prediction\n",
      "finished 2490 data prediction\n",
      "finished 2520 data prediction\n",
      "finished 2550 data prediction\n",
      "finished 2580 data prediction\n",
      "finished 2610 data prediction\n",
      "finished 2640 data prediction\n",
      "finished 2670 data prediction\n",
      "finished 2700 data prediction\n",
      "finished 2730 data prediction\n",
      "finished 2760 data prediction\n",
      "finished 2790 data prediction\n",
      "finished 2820 data prediction\n",
      "finished 2850 data prediction\n",
      "finished 2880 data prediction\n",
      "finished 2910 data prediction\n",
      "finished 2940 data prediction\n",
      "finished 2970 data prediction\n",
      "finished 3000 data prediction\n",
      "finished 3030 data prediction\n",
      "finished 3060 data prediction\n",
      "finished 3090 data prediction\n",
      "finished 3120 data prediction\n",
      "finished 3150 data prediction\n",
      "finished 3180 data prediction\n",
      "finished 3210 data prediction\n",
      "finished 3240 data prediction\n",
      "finished 3270 data prediction\n",
      "finished 3300 data prediction\n",
      "finished 3330 data prediction\n",
      "finished 3360 data prediction\n",
      "finished 3390 data prediction\n",
      "finished 3420 data prediction\n",
      "finished 3450 data prediction\n",
      "finished 3480 data prediction\n",
      "finished 3510 data prediction\n",
      "finished 3540 data prediction\n",
      "finished 3570 data prediction\n",
      "finished 3600 data prediction\n",
      "finished 3630 data prediction\n",
      "finished 3660 data prediction\n",
      "finished 3690 data prediction\n",
      "finished 3720 data prediction\n",
      "finished 3750 data prediction\n",
      "finished 3780 data prediction\n",
      "finished 3810 data prediction\n",
      "finished 3840 data prediction\n",
      "finished 3870 data prediction\n",
      "finished 3900 data prediction\n",
      "finished 3930 data prediction\n",
      "finished 3960 data prediction\n",
      "finished 3990 data prediction\n",
      "finished 4020 data prediction\n",
      "finished 4050 data prediction\n",
      "finished 4080 data prediction\n",
      "finished 4110 data prediction\n",
      "finished 4140 data prediction\n",
      "finished 4170 data prediction\n",
      "finished 4200 data prediction\n",
      "finished 4230 data prediction\n",
      "finished 4260 data prediction\n",
      "finished 4290 data prediction\n",
      "finished 4320 data prediction\n",
      "finished 4350 data prediction\n",
      "finished 4380 data prediction\n",
      "finished 4410 data prediction\n",
      "finished 4440 data prediction\n",
      "finished 4470 data prediction\n",
      "finished 4500 data prediction\n",
      "finished 4530 data prediction\n",
      "finished 4560 data prediction\n",
      "finished 4590 data prediction\n",
      "finished 4620 data prediction\n",
      "finished 4650 data prediction\n",
      "finished 4680 data prediction\n",
      "finished 4710 data prediction\n",
      "finished 4740 data prediction\n",
      "finished 4770 data prediction\n",
      "finished 4800 data prediction\n",
      "finished 4830 data prediction\n",
      "finished 4860 data prediction\n",
      "finished 4890 data prediction\n",
      "finished 4920 data prediction\n",
      "finished 4950 data prediction\n",
      "finished 4980 data prediction\n",
      "finished 5010 data prediction\n",
      "finished 5040 data prediction\n",
      "finished 5070 data prediction\n",
      "finished 5100 data prediction\n",
      "finished 5130 data prediction\n",
      "finished 5160 data prediction\n",
      "finished 5190 data prediction\n",
      "finished 5220 data prediction\n",
      "finished 5250 data prediction\n",
      "finished 5280 data prediction\n",
      "finished 5310 data prediction\n",
      "finished 5340 data prediction\n",
      "finished 5370 data prediction\n",
      "finished 5400 data prediction\n",
      "finished 5430 data prediction\n",
      "finished 5460 data prediction\n",
      "finished 5490 data prediction\n",
      "finished 5520 data prediction\n",
      "finished 5550 data prediction\n",
      "finished 5580 data prediction\n",
      "finished 5610 data prediction\n",
      "finished 5640 data prediction\n",
      "finished 5670 data prediction\n",
      "finished 5700 data prediction\n",
      "finished 5730 data prediction\n",
      "finished 5760 data prediction\n",
      "finished 5790 data prediction\n",
      "finished 5820 data prediction\n",
      "finished 5850 data prediction\n",
      "finished 5880 data prediction\n",
      "finished 5910 data prediction\n",
      "finished 5940 data prediction\n",
      "finished 5970 data prediction\n",
      "finished 6000 data prediction\n",
      "finished 6030 data prediction\n",
      "finished 6060 data prediction\n",
      "finished 6090 data prediction\n",
      "finished 6120 data prediction\n",
      "finished 6150 data prediction\n",
      "finished 6180 data prediction\n",
      "finished 6210 data prediction\n",
      "finished 6240 data prediction\n",
      "finished 6270 data prediction\n",
      "finished 6300 data prediction\n",
      "finished 6330 data prediction\n",
      "finished 6360 data prediction\n",
      "finished 6390 data prediction\n",
      "finished 6420 data prediction\n",
      "finished 6450 data prediction\n",
      "finished 6480 data prediction\n",
      "finished 6510 data prediction\n",
      "finished 6540 data prediction\n",
      "finished 6570 data prediction\n",
      "finished 6600 data prediction\n",
      "finished 6630 data prediction\n",
      "finished 6660 data prediction\n",
      "finished 6690 data prediction\n",
      "finished 6720 data prediction\n",
      "finished 6750 data prediction\n",
      "finished 6780 data prediction\n",
      "finished 6810 data prediction\n",
      "finished 6840 data prediction\n",
      "finished 6870 data prediction\n",
      "finished 6900 data prediction\n",
      "finished 6930 data prediction\n",
      "finished 6960 data prediction\n",
      "finished 6990 data prediction\n",
      "finished 7020 data prediction\n",
      "finished 7050 data prediction\n",
      "finished 7080 data prediction\n",
      "finished 7110 data prediction\n",
      "finished 7140 data prediction\n",
      "finished 7170 data prediction\n",
      "finished 7200 data prediction\n",
      "finished 7230 data prediction\n",
      "finished 7260 data prediction\n",
      "finished 7290 data prediction\n",
      "finished 7320 data prediction\n",
      "finished 7350 data prediction\n",
      "finished 7380 data prediction\n",
      "finished 7410 data prediction\n",
      "finished 7440 data prediction\n",
      "finished 7470 data prediction\n",
      "finished 7500 data prediction\n",
      "finished 7530 data prediction\n",
      "finished 7560 data prediction\n",
      "finished 7590 data prediction\n",
      "finished 7620 data prediction\n",
      "finished 7650 data prediction\n",
      "finished 7680 data prediction\n",
      "finished 7710 data prediction\n",
      "finished 7740 data prediction\n",
      "finished 7770 data prediction\n",
      "finished 7800 data prediction\n",
      "finished 7830 data prediction\n",
      "finished 7860 data prediction\n",
      "finished 7890 data prediction\n",
      "finished 7920 data prediction\n",
      "finished 7950 data prediction\n",
      "finished 7980 data prediction\n",
      "finished 8010 data prediction\n",
      "finished 8040 data prediction\n",
      "finished 8070 data prediction\n",
      "finished 8100 data prediction\n",
      "finished 8130 data prediction\n",
      "finished 8160 data prediction\n",
      "finished 8190 data prediction\n",
      "finished 8220 data prediction\n",
      "finished 8250 data prediction\n",
      "finished 8280 data prediction\n",
      "finished 8310 data prediction\n",
      "finished 8340 data prediction\n",
      "finished 8370 data prediction\n",
      "finished 8400 data prediction\n",
      "finished 8430 data prediction\n",
      "finished 8460 data prediction\n",
      "finished 8490 data prediction\n",
      "finished 8520 data prediction\n",
      "finished 8550 data prediction\n",
      "finished 8580 data prediction\n",
      "finished 8610 data prediction\n",
      "finished 8640 data prediction\n",
      "finished 8670 data prediction\n",
      "finished 8700 data prediction\n",
      "finished 8730 data prediction\n",
      "finished 8760 data prediction\n",
      "finished 8790 data prediction\n",
      "finished 8820 data prediction\n",
      "finished 8850 data prediction\n",
      "finished 8880 data prediction\n",
      "finished 8910 data prediction\n",
      "finished 8940 data prediction\n",
      "finished 8970 data prediction\n",
      "finished 9000 data prediction\n",
      "finished 9030 data prediction\n",
      "finished 9060 data prediction\n",
      "finished 9090 data prediction\n",
      "finished 9120 data prediction\n",
      "finished 9150 data prediction\n",
      "finished 9180 data prediction\n",
      "finished 9210 data prediction\n",
      "finished 9240 data prediction\n",
      "finished 9270 data prediction\n",
      "finished 9300 data prediction\n",
      "finished 9330 data prediction\n",
      "finished 9360 data prediction\n",
      "finished 9390 data prediction\n",
      "finished 9420 data prediction\n",
      "finished 9450 data prediction\n",
      "finished 9480 data prediction\n",
      "finished 9510 data prediction\n",
      "finished 9540 data prediction\n",
      "finished 9570 data prediction\n",
      "finished 9600 data prediction\n",
      "finished 9630 data prediction\n",
      "finished 9660 data prediction\n",
      "finished 9690 data prediction\n",
      "finished 9720 data prediction\n",
      "finished 9750 data prediction\n",
      "finished 9780 data prediction\n",
      "finished 9810 data prediction\n",
      "finished 9840 data prediction\n",
      "finished 9870 data prediction\n",
      "finished 9900 data prediction\n",
      "finished 9930 data prediction\n",
      "finished 9960 data prediction\n",
      "finished 9990 data prediction\n",
      "finished 10020 data prediction\n",
      "finished 10050 data prediction\n",
      "finished 10080 data prediction\n",
      "finished 10110 data prediction\n",
      "finished 10140 data prediction\n",
      "finished 10170 data prediction\n",
      "finished 10200 data prediction\n",
      "finished 10230 data prediction\n",
      "finished 10260 data prediction\n",
      "finished 10290 data prediction\n",
      "finished 10320 data prediction\n",
      "finished 10350 data prediction\n",
      "finished 10380 data prediction\n",
      "finished 10410 data prediction\n",
      "finished 10440 data prediction\n",
      "finished 10470 data prediction\n",
      "finished 10500 data prediction\n",
      "finished 10530 data prediction\n",
      "finished 10560 data prediction\n",
      "finished 10590 data prediction\n",
      "finished 10620 data prediction\n",
      "finished 10650 data prediction\n",
      "finished 10680 data prediction\n",
      "finished 10710 data prediction\n",
      "finished 10740 data prediction\n",
      "finished 10770 data prediction\n",
      "finished 10800 data prediction\n",
      "finished 10830 data prediction\n",
      "finished 10860 data prediction\n",
      "finished 10890 data prediction\n",
      "finished 10920 data prediction\n",
      "finished 10950 data prediction\n",
      "finished 10980 data prediction\n",
      "finished 11010 data prediction\n",
      "finished 11040 data prediction\n",
      "finished 11070 data prediction\n",
      "finished 11100 data prediction\n",
      "finished 11130 data prediction\n",
      "finished 11160 data prediction\n",
      "finished 11190 data prediction\n",
      "finished 11220 data prediction\n",
      "finished 11250 data prediction\n",
      "finished 11280 data prediction\n",
      "finished 11310 data prediction\n",
      "finished 11340 data prediction\n",
      "finished 11370 data prediction\n",
      "finished 11400 data prediction\n",
      "finished 11430 data prediction\n",
      "finished 11460 data prediction\n",
      "finished 11490 data prediction\n",
      "finished 11520 data prediction\n",
      "finished 11550 data prediction\n",
      "finished 11580 data prediction\n",
      "finished 11610 data prediction\n",
      "finished 11640 data prediction\n",
      "finished 11670 data prediction\n",
      "finished 11700 data prediction\n",
      "finished 11730 data prediction\n",
      "finished 11760 data prediction\n",
      "finished 11790 data prediction\n",
      "finished 11820 data prediction\n",
      "finished 11850 data prediction\n",
      "finished 11880 data prediction\n",
      "finished 11910 data prediction\n",
      "finished 11940 data prediction\n",
      "finished 11970 data prediction\n",
      "finished 12000 data prediction\n",
      "finished 12030 data prediction\n",
      "finished 12060 data prediction\n",
      "finished 12090 data prediction\n",
      "finished 12120 data prediction\n",
      "finished 12150 data prediction\n",
      "finished 12180 data prediction\n",
      "finished 12210 data prediction\n",
      "finished 12240 data prediction\n",
      "finished 12270 data prediction\n",
      "finished 12300 data prediction\n",
      "finished 12330 data prediction\n",
      "finished 12360 data prediction\n",
      "finished 12390 data prediction\n",
      "finished 12420 data prediction\n",
      "finished 12450 data prediction\n",
      "finished 12480 data prediction\n",
      "finished 12510 data prediction\n",
      "finished 12540 data prediction\n",
      "finished 12570 data prediction\n",
      "finished 12600 data prediction\n",
      "finished 12630 data prediction\n",
      "finished 12660 data prediction\n",
      "finished 12690 data prediction\n",
      "finished 12720 data prediction\n",
      "finished 12750 data prediction\n",
      "finished 12780 data prediction\n",
      "finished 12810 data prediction\n",
      "finished 12840 data prediction\n",
      "finished 12870 data prediction\n",
      "finished 12900 data prediction\n",
      "finished 12930 data prediction\n",
      "finished 12960 data prediction\n",
      "finished 12990 data prediction\n",
      "finished 13020 data prediction\n",
      "finished 13050 data prediction\n",
      "finished 13080 data prediction\n",
      "finished 13110 data prediction\n",
      "finished 13140 data prediction\n",
      "finished 13170 data prediction\n",
      "finished 13200 data prediction\n",
      "finished 13230 data prediction\n",
      "finished 13260 data prediction\n",
      "finished 13290 data prediction\n",
      "finished 13320 data prediction\n",
      "finished 13350 data prediction\n",
      "finished 13380 data prediction\n",
      "finished 13410 data prediction\n",
      "finished 13440 data prediction\n",
      "finished 13470 data prediction\n",
      "finished 13500 data prediction\n",
      "finished 13530 data prediction\n",
      "finished 13560 data prediction\n",
      "finished 13590 data prediction\n",
      "finished 13620 data prediction\n",
      "finished 13650 data prediction\n",
      "finished 13680 data prediction\n",
      "finished 13710 data prediction\n",
      "finished 13740 data prediction\n",
      "finished 13770 data prediction\n",
      "finished 13800 data prediction\n",
      "finished 13830 data prediction\n",
      "finished 13860 data prediction\n",
      "finished 13890 data prediction\n",
      "finished 13920 data prediction\n",
      "finished 13950 data prediction\n",
      "finished 13980 data prediction\n",
      "finished 14010 data prediction\n",
      "finished 14040 data prediction\n",
      "finished 14070 data prediction\n",
      "finished 14100 data prediction\n",
      "finished 14130 data prediction\n",
      "finished 14160 data prediction\n",
      "finished 14190 data prediction\n",
      "finished 14220 data prediction\n",
      "finished 14250 data prediction\n",
      "finished 14280 data prediction\n",
      "finished 14310 data prediction\n",
      "finished 14340 data prediction\n",
      "finished 14370 data prediction\n",
      "finished 14400 data prediction\n",
      "finished 14430 data prediction\n",
      "finished 14460 data prediction\n",
      "finished 14490 data prediction\n",
      "0.8152745969168435 0.9087586206896552\n"
     ]
    }
   ],
   "source": [
    "micro,macro=bert_inference_f1score()\n",
    "print(micro,macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from dataset_extractor import *\n",
    "import torch\n",
    "def bert_inference_regression(batchsize=120):\n",
    "    train_pair=extract(\"/data/chenhaohua/PRC_legal_dataset/data_valid.json\")\n",
    "    device=torch.device(\"cuda\",4)\n",
    "    model_trained.to(device) # model to device once\n",
    "    all_predictions=[]\n",
    "    all_ground_truth=[]\n",
    "    for i in range(0,len(train_pair['content']),batchsize):\n",
    "        end_index = min(i + batchsize, len(train_pair['content'])-1)\n",
    "        infer_batch_tokens=tokenizer(train_pair['content'][i:end_index],padding=True,return_tensors='pt').to(device)\n",
    "        output=model_trained(**infer_batch_tokens)\n",
    "        output_logits = output.logits.to(\"cpu\")#must return specific value\n",
    "        prediction=output_logits.detach().numpy().reshape(-1)\n",
    "        ground_truth=np.array(train_pair[\"imprisonment\"][i:end_index])\n",
    "        all_predictions.extend(prediction)\n",
    "        all_ground_truth.extend(ground_truth)\n",
    "        if i%batchsize==0 and i!=0:\n",
    "            print(f\"finished {i} data prediction\")\n",
    "    errors=np.absolute(np.log(np.array(all_predictions)+1)-np.log(np.array(all_ground_truth)+1))\n",
    "    score=0\n",
    "    for error in errors:\n",
    "        if error <= 0.2:\n",
    "            score += 1.0\n",
    "        elif error <= 0.4:\n",
    "            score += 0.8\n",
    "        elif error <= 0.6:\n",
    "            score += 0.6\n",
    "        elif error <= 0.8:\n",
    "            score += 0.4\n",
    "        elif error <= 1.0:\n",
    "            score += 0.2\n",
    "    return score*100/len(train_pair['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14501 lines have been extracted\n",
      "finished 120 data prediction\n",
      "finished 240 data prediction\n",
      "finished 360 data prediction\n",
      "finished 480 data prediction\n",
      "finished 600 data prediction\n",
      "finished 720 data prediction\n",
      "finished 840 data prediction\n",
      "finished 960 data prediction\n",
      "finished 1080 data prediction\n",
      "finished 1200 data prediction\n",
      "finished 1320 data prediction\n",
      "finished 1440 data prediction\n",
      "finished 1560 data prediction\n",
      "finished 1680 data prediction\n",
      "finished 1800 data prediction\n",
      "finished 1920 data prediction\n",
      "finished 2040 data prediction\n",
      "finished 2160 data prediction\n",
      "finished 2280 data prediction\n",
      "finished 2400 data prediction\n",
      "finished 2520 data prediction\n",
      "finished 2640 data prediction\n",
      "finished 2760 data prediction\n",
      "finished 2880 data prediction\n",
      "finished 3000 data prediction\n",
      "finished 3120 data prediction\n",
      "finished 3240 data prediction\n",
      "finished 3360 data prediction\n",
      "finished 3480 data prediction\n",
      "finished 3600 data prediction\n",
      "finished 3720 data prediction\n",
      "finished 3840 data prediction\n",
      "finished 3960 data prediction\n",
      "finished 4080 data prediction\n",
      "finished 4200 data prediction\n",
      "finished 4320 data prediction\n",
      "finished 4440 data prediction\n",
      "finished 4560 data prediction\n",
      "finished 4680 data prediction\n",
      "finished 4800 data prediction\n",
      "finished 4920 data prediction\n",
      "finished 5040 data prediction\n",
      "finished 5160 data prediction\n",
      "finished 5280 data prediction\n",
      "finished 5400 data prediction\n",
      "finished 5520 data prediction\n",
      "finished 5640 data prediction\n",
      "finished 5760 data prediction\n",
      "finished 5880 data prediction\n",
      "finished 6000 data prediction\n",
      "finished 6120 data prediction\n",
      "finished 6240 data prediction\n",
      "finished 6360 data prediction\n",
      "finished 6480 data prediction\n",
      "finished 6600 data prediction\n",
      "finished 6720 data prediction\n",
      "finished 6840 data prediction\n",
      "finished 6960 data prediction\n",
      "finished 7080 data prediction\n",
      "finished 7200 data prediction\n",
      "finished 7320 data prediction\n",
      "finished 7440 data prediction\n",
      "finished 7560 data prediction\n",
      "finished 7680 data prediction\n",
      "finished 7800 data prediction\n",
      "finished 7920 data prediction\n",
      "finished 8040 data prediction\n",
      "finished 8160 data prediction\n",
      "finished 8280 data prediction\n",
      "finished 8400 data prediction\n",
      "finished 8520 data prediction\n",
      "finished 8640 data prediction\n",
      "finished 8760 data prediction\n",
      "finished 8880 data prediction\n",
      "finished 9000 data prediction\n",
      "finished 9120 data prediction\n",
      "finished 9240 data prediction\n",
      "finished 9360 data prediction\n",
      "finished 9480 data prediction\n",
      "finished 9600 data prediction\n",
      "finished 9720 data prediction\n",
      "finished 9840 data prediction\n",
      "finished 9960 data prediction\n",
      "finished 10080 data prediction\n",
      "finished 10200 data prediction\n",
      "finished 10320 data prediction\n",
      "finished 10440 data prediction\n",
      "finished 10560 data prediction\n",
      "finished 10680 data prediction\n",
      "finished 10800 data prediction\n",
      "finished 10920 data prediction\n",
      "finished 11040 data prediction\n",
      "finished 11160 data prediction\n",
      "finished 11280 data prediction\n",
      "finished 11400 data prediction\n",
      "finished 11520 data prediction\n",
      "finished 11640 data prediction\n",
      "finished 11760 data prediction\n",
      "finished 11880 data prediction\n",
      "finished 12000 data prediction\n",
      "finished 12120 data prediction\n",
      "finished 12240 data prediction\n",
      "finished 12360 data prediction\n",
      "finished 12480 data prediction\n",
      "finished 12600 data prediction\n",
      "finished 12720 data prediction\n",
      "finished 12840 data prediction\n",
      "finished 12960 data prediction\n",
      "finished 13080 data prediction\n",
      "finished 13200 data prediction\n",
      "finished 13320 data prediction\n",
      "finished 13440 data prediction\n",
      "finished 13560 data prediction\n",
      "finished 13680 data prediction\n",
      "finished 13800 data prediction\n",
      "finished 13920 data prediction\n",
      "finished 14040 data prediction\n",
      "finished 14160 data prediction\n",
      "finished 14280 data prediction\n",
      "finished 14400 data prediction\n",
      "63.20529618647122\n"
     ]
    }
   ],
   "source": [
    "model_trained=AutoModelForSequenceClassification.from_pretrained('/data/chenhaohua/PRC_legal/bert-base-legal-chinese-regression-epoch-24')\n",
    "model_trained.eval()\n",
    "tokenizer=AutoTokenizer.from_pretrained('/data/chenhaohua/PRC_legal/bert-base-chinese')\n",
    "with torch.no_grad():\n",
    "    result=bert_inference_regression()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
